{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b990eeca-2723-4fbf-a2f4-249552d513e6",
   "metadata": {},
   "source": [
    "## DDPM on CIFAR10\n",
    "\n",
    "This is just me trying to learn how to implement simple diffusion models\n",
    "\n",
    "paper|official code\n",
    "-|-\n",
    "[Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) | [code](https://github.com/hojonathanho/diffusion)\n",
    "[Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672) | [code](https://github.com/openai/improved-diffusion)\n",
    "[Diffusion Models Beat GANs on Image Synthesis](https://arxiv.org/abs/2105.05233) | [code](https://github.com/openai/guided-diffusion)\n",
    "\n",
    "helpful resources\n",
    "\n",
    "- [lucidrains full implementation in one file](https://github.com/lucidrains/denoising-diffusion-pytorch/blob/7706bdfc6f527f58d33f84b7b522e61e6e3164b3/denoising_diffusion_pytorch/denoising_diffusion_pytorch.py)\n",
    "- [unet parts](https://github.com/milesial/Pytorch-UNet/blob/master/unet/unet_parts.py)\n",
    "\n",
    "\n",
    "## What's next?\n",
    "\n",
    "- maybe text diffusion? https://github.com/madaan/minimal-text-diffusiony"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8dafd2-a480-4148-aa03-bfb7bb74020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install wandb easydict tqdm torch==2 torchvision -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82af6a4-f4b9-47b1-b7d9-e152509ed384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prefix_dict(prefix, dic):\n",
    "    return {f\"{prefix}.{key}\": value for key, value in dic.items()}\n",
    "def flatten_dict(d, parent_key='', sep='.'):\n",
    "    items = {}\n",
    "    for k, v in d.items():\n",
    "        new_key = f\"{parent_key}{sep}{k}\" if parent_key else k\n",
    "        if isinstance(v, dict):\n",
    "            items.update(flatten_dict(v, new_key, sep=sep))\n",
    "        else:\n",
    "            items[new_key] = v\n",
    "    return items\n",
    "\n",
    "\n",
    "class Timer:\n",
    "    def __init__(self, message=''):\n",
    "        self.message = message\n",
    "    def __enter__(self):\n",
    "        self.start_time = time.time()\n",
    "        return self\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        self.end_time = time.time()\n",
    "        elapsed_time = self.end_time - self.start_time\n",
    "        # print(f\"{self.message} time: {elapsed_time:.2f}sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4745b3b-45c5-4c8b-a985-0c229a2c91ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "assert device == 'cuda'\n",
    "# torch.set_default_device(device)\n",
    "\n",
    "# optimizer = torch.optim.Adam(model.parameters())\n",
    "# criterion = torch.nn.MSELoss().cuda()\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    # transforms.ColorJitter(brightness=0.5,contrast=0.5),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ##TODO: should I use this?\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1),\n",
    "])\n",
    "\n",
    "batch_size = 512\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(os.path.expanduser(\"~/data/\"), download=True, train=True, transform=transform_train)\n",
    "test_dataset = torchvision.datasets.CIFAR10(os.path.expanduser(\"~/data/\"), download=True, train=False, transform=transform_test)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=19, prefetch_factor=2,)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=19, prefetch_factor=2,)\n",
    "\n",
    "import time\n",
    "tic = time.time()\n",
    "\n",
    "for i, x in enumerate(tqdm(train_loader)):\n",
    "    if i > 50:\n",
    "        break\n",
    "\n",
    "duration = time.time() - tic\n",
    "\n",
    "print(f'dataloader completed 10 batches in {duration:.2f}')\n",
    "assert duration < 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ed2916-de26-4c9b-a7e1-836491fa4ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = (img + 1) / 2     # unnormalize\n",
    "    npimg = img.detach().cpu().numpy().astype(float)\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "imshow(train_dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7775940-cab2-42f0-8a05-c1bdfe2359b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import torch.nn.functional as F\n",
    "from typing import *\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(\n",
    "            self, \n",
    "            in_channels: int,\n",
    "            out_channels: int,\n",
    "            kernel_size: Union[int, Tuple[int, int]]=3,\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv2d(in_channels, out_channels//2, kernel_size, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels//2, out_channels, kernel_size, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "    def forward(x):\n",
    "        return self.convs(x)\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]]=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(dim, dim, 3, 2, 1)\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(\n",
    "        self, \n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: Union[int, Tuple[int, int]]=3,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.convs = DoubleConv(in_channels, out_channels, kernel_size)\n",
    "        self.time_mlp = nn.ModuleList([\n",
    "            nn.Linear(1, kernel_size**2//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(kernel_size**2//2, kernel_size**2),\n",
    "            nn.ReLU(inplace=True),\n",
    "        ])\n",
    "    def forward(self, x, t=None):\n",
    "        x = self.convs(x)\n",
    "        if t is not None:\n",
    "            x = torch.concat((x, self.time_mlp(t)), dim=0)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, bias=True, kernel_size=3, stride=1, final_conv=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 128, kernel_size=kernel_size, stride=stride, padding=1, bias=bias)\n",
    "        self.conv2 = nn.Conv2d(128, 512, kernel_size=kernel_size, stride=stride, padding=1, bias=bias)\n",
    "        self.conv3 = nn.Conv2d(512, 2048, kernel_size=kernel_size, stride=stride, padding=1, bias=bias)\n",
    "        self.deconv1 = nn.ConvTranspose2d(2048, 512, kernel_size=kernel_size, stride=stride, padding=1, bias=bias)\n",
    "        self.deconv2 = nn.ConvTranspose2d(512, 128, kernel_size=kernel_size, stride=stride, padding=1, bias=bias)\n",
    "        self.deconv3 = nn.ConvTranspose2d(128, 3, kernel_size=kernel_size, stride=stride, padding=1, bias=bias)\n",
    "\n",
    "        self.final_conv = None\n",
    "        if final_conv:\n",
    "            self.final_conv = nn.Conv2d(3, 3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(F.relu(x1))\n",
    "        x3 = self.conv3(F.relu(x2))\n",
    "\n",
    "        x4 = self.deconv1(F.relu(x3)) + x2\n",
    "        x5 = self.deconv2(F.relu(x4)) + x1\n",
    "        x6 = self.deconv3(F.relu(x5)) + x\n",
    "\n",
    "        if self.final_conv is not None:\n",
    "            x6 = self.final_conv(x6)\n",
    "        return F.tanh(x6)\n",
    "\n",
    "\n",
    "model_config = {\n",
    "    \"bias\": True,\n",
    "    \"kernel_size\": 3,\n",
    "    \"stride\": 1,\n",
    "    \"final_conv\": False,\n",
    "}\n",
    "\n",
    "# test\n",
    "net = Unet(**model_config)\n",
    "\n",
    "inp = torch.ones(1, 3, 10, 10)\n",
    "output = net(inp)\n",
    "assert output.shape == inp.shape\n",
    "del net\n",
    "print('network tested successfully')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "lessons:\n",
    "\n",
    "padding will change the size\n",
    "\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ce58d5-a5f1-4fe8-8b2b-bd3c7ee16687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pprint\n",
    "from easydict import EasyDict\n",
    "\n",
    "global_step = 0\n",
    "\n",
    "config = EasyDict({\n",
    "    \"use_amp\": True,\n",
    "    \"epochs\": 5,\n",
    "    \"optimizer\": \"Adam\",\n",
    "    \"optimizer_kwargs\": {\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"loss_fn\": \"MSELoss\",\n",
    "    \"eval_step\": 20,\n",
    "    \"batch_size\": batch_size,\n",
    "    # \"eval_step\": eval_step,\n",
    "    \"model\": model_config,\n",
    "    # any other hyperparameters or settings you want to save\n",
    "})\n",
    "\n",
    "\n",
    "net = Unet(**model_config).cuda() #make_model(in_size, out_size, num_layers)\n",
    "opt = getattr(torch.optim, config.optimizer)(net.parameters(), **config.optimizer_kwargs)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=config.use_amp)\n",
    "loss_fn = getattr(nn, config.loss_fn)().to(device)\n",
    "\n",
    "pprint.pprint(config)\n",
    "wandb.finish()\n",
    "# Initialize wandb\n",
    "wandb.init(project='DDPM from scratch', name='3layer unet identity function', config=flatten_dict(config), resume=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a3ef4e-dcb3-4e1c-8523-886204d8a1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# wandb.init(project='DDPM from scratch', name='3layer unet identity function', config=flatten_dict(config), resume=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7008f0-c9b2-4196-bf64-4c86532d5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    # for input, target in zip(tqdm(data), targets):\n",
    "    pbar = tqdm(train_loader)\n",
    "    start_time = time.time()\n",
    "    for b, (input, target) in enumerate(pbar):\n",
    "        # print(f'dataloader time: {time.time()-start_time}sec')\n",
    "        global_step += 1\n",
    "        # target = input.clone().cuda()\n",
    "        input = input.cuda()\n",
    "\n",
    "        with torch.autocast(device_type=device, dtype=torch.float16, enabled=config.use_amp):\n",
    "            with Timer(\"forward\"):\n",
    "                output = net(input)\n",
    "            with Timer(\"loss_fn\"):\n",
    "                loss = loss_fn(output, input)\n",
    "                wandb.log({'loss': loss.item()})\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "        if global_step % config.eval_step == config.eval_step-1:\n",
    "            val_sample = test_dataset[0][0].unsqueeze(0).to(device)\n",
    "            val_output = net(val_sample)\n",
    "            img_sample = torch.concat((val_sample[0], val_output[0], val_sample[0] - val_output[0]), dim=2)\n",
    "            imshow(img_sample)\n",
    "            image = wandb.Image(\n",
    "                img_sample, \n",
    "                caption=\"Left: original, Right: reconstructed\"\n",
    "            )\n",
    "            wandb.log({\"examples\": image})\n",
    "        with Timer(\"backward\"):\n",
    "            scaler.scale(loss).backward()\n",
    "        with Timer(\"step\"):\n",
    "            scaler.step(opt)\n",
    "        scaler.update()\n",
    "        opt.zero_grad() # set_to_none=True here can modestly improve performance\n",
    "        start_time = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939cb51e-b24f-4051-be61-2a1f9e901758",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt2]",
   "language": "python",
   "name": "conda-env-pt2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
